<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<style>
div.ml1 {
  margin-left: 30px;
}
ul.a {list-style-type: circle;}
</style>

<html>
	<head>
		<title>MISP Challenge-Task2 Software</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">

					<!-- Logo -->
					<img src="images/isca.png" width="6%" height="6%" style="margin:0 77% 0 0;"><h1 style="font-size: 120%;  margin: -4% 0 -2% 0"><a href="index.html" id="logo">Multimodal Information Based Speech Processing (MISP) Challenge 2022</a><img src="images/ieee.png" width="7%" height="7%" style="margin:-15% 0% 1% 79%;"></h1>
					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="overview.html">Overview</a><li>
								<li>
									<a href="#">Task1</a>
									<ul>
										<li><a href="task1_instructions.html">Instructions</a></li>										
										<li><a href="task1_data.html">Data</a></li>
										<li><a href="task1_software.html">Software</a></li>
						                <li><a href="task1_submission.html">Submission</a></li>
									</ul>
								</li>
								<li class="current">
									<a href="#">Task2</a>
									<ul>
										<li><a href="task2_instructions.html">Instructions</a></li>										
										<li><a href="task2_data.html">Data</a></li>
										<li><a href="task2_software.html">Software</a></li>
						                <li><a href="task2_submission.html">Submission</a></li>
									</ul>
								</li>
								<li><a href="download.html">Registration</a></li>
								<!-- <li><a href="extral_data.html">Extral Data</a></li> -->
								<!-- <li><a href="results.html">Results</a></li> -->
								<!-- <li><a href="contact.html">Contact</a></li> -->
							</ul>
						</nav>

				</div>

			<!-- Main -->
			<section class="wrapper style1">
					<div class="container">
						<h1 style="font-size: 150%;">Task2 NN-HMM Based AVSR Baseline</h1>
						<!-- <p>For details, please refer to <a href="https://github.com/mispchallenge/misp2021_baseline/tree/master/task2_avsr_nn_hmm"> Github link </a></p>
						<ul>
						<li>
						<p><strong>Data preparation</strong></p>
						<ul class="a">
						<li><strong>speech enhancement</strong></li>
						</ul>
						<p>We provide two baseline speech enhancement front-ends, Weighted Prediction Error(WPE) dereverberation and  weighted delay-and-sum(DAS) beamforming, to reduce reverberations and noises of speech signals. These two algorithms are implemented with open-source toolkits, <a href="https://github.com/fgnt/nara_wpe">nara_wpe</a> and <a href="https://github.com/xanguera/BeamformIt">BeamformIt</a>, respectively.</p>
						<ul class="a">
						<li><strong>prepare data and language directory for kaldi</strong></li>
						</ul>
						<p>For training, development, and test sets, we prepare data directories and the lexicon in the format expected by  <a href="http://kaldi-asr.org/doc/data_prep.html">kaldi</a> respectively. Note that we choose <a href="https://github.com/aishell-foundation/DaCiDian.git">DaCiDian</a> raw resource and convert it to kaldi lexicon format.</p>
						</li>
						<li>
						<p><strong>Language model</strong></p>
						<p>We segment MISP speech transcription for language model training by applying <a href="https://github.com/aishell-foundation/DaCiDian.git">DaCiDian</a> as dict and <a href="https://github.com/fxsjy/jieba">Jieba</a> open-source toolkit. For the language model, we choose a maximum entropy-based 3-gram model, which achieves the best perplexity, from n-gram(n=2,3,4) models trained on MISP speech transcripts with different smoothing algorithms and parameters sets. And the selected 3-gram model has 516600 unigrams, 432247 bigrams, and 915962 trigrams respectively.  Note that the temporary and final language models are stored in /data/srilm.</p>
						</li>
						<li>
						<p><strong>Acoustic model</strong></p>
						<p>The acoustic model of the ASR system is built largely following the Kaldi <a href="https://github.com/kaldi-asr/kaldi/tree/master/egs/chime6/s5_track1">CHIME6</a> recipes which mainly contain two stages: GMM-HMM state model and TDNN deep learning model.</p>
						<ul class="a">
						<li>
						<p><strong>GMM-HMM</strong></p>
						<p>For features extraction, we extract 13-dimensional MFCC features plus 3-dimensional pitches. As a start point for triphone models, a monophone model is trained on a subset of 50k utterances.  Then a small triphone model and a larger triphone model are consecutively trained using delta features on a subset of 100k utterances and the whole dataset respectively. In the third triphone model training process, an MLLT-based global transform is estimated iteratively on the top of LDA feature to extract independent speaker features. For the fourth triphone model, feature space maximum likelihood linear regression (fMLLR) with speaker adaptive training (SAT) is applied in the training.</p>
						</li>
						<li>
						<p><strong>NN-HMM</strong></p>
						<p>Based on the tied-triphone state alignments from GMM, TDNN is configured and trained to replace GMM. Here two data augmentation technologies, speed-perturbation and volume-perturbation are applied on signal level. The input features are 40-dimensional high-resolution MFCC features with cepstral normalization. Note that for each frame-wise input, a 100-dimensional i-vector is also attached, whose extractor was trained on the expanded corpus. An advanced time-delayed neural network (TDNN) baseline using lattice-free maximum mutual information (LF-MMI) training and other strategies is adopted in the system, and you can consult the <a href="https://www.danielpovey.com/files/2018_interspeech_tdnnf.pdf">paper</a> and the <a href="https://kaldi-asr.org/doc/chain.html">document</a> for more details.</p>
						</li>
						</ul>
						</li>
						<li>
						<p><strong>Audio-Visual Speech Recognition</strong></p>
						<p>Based on the  NN-HMM hybrid ASR system mentioned above, we try to use lipreading to enhance the system. To get visual embeddings, we firstly crop mouth ROIs from video streams, then use the <a href="https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks">lipreading TCN</a>  to extract 512-dimensional features. Still using TDNN, we simply concentrate two modals embeddings(512+40+100) in the training stage. And the results preliminary show that the video information does help enhance ASR and it still has a very big upgrade space. Here comes the results.</p>
						</li>
						</ul>
						<p></p> -->
					</div>
				</section>

				<section class="wrapper style2">
					<div class="container">
						<h1 style="font-size: 120%;">Quick start</h1>
						<!-- <ul><li><strong>Setting Local System Jobs</strong></li></ul>
						<div class="ml1">
						<p><code># Setting local system jobs (local CPU - no external clusters)</br>
						export train_cmd=run.pl</br>
						export decode_cmd=run.pl</code></p>
					</div>
						<p></p>
						<ul>
						<li><strong>Setting  Paths</strong></li>
						</ul>
						<div class="ml1">
						<p><code>--- path.sh ---</br>
						# Defining Kaldi root directory</br>
						export KALDI_ROOT=</br>
						# Setting paths to useful tools</br>
						export PATH=</br>
						# Enable SRILM</br>
						. $KALDI_ROOT/tools/env.sh</br>
						# Variable needed for proper data sorting</br>
						export LC_ALL=C</p>

						<p>--- run.sh ---</br>
						# Defining corpus directory</br>
						misp2021_corpus=</br>
						# Defining path to beamforIt executable file</br>
						bearmformit_path = </br>
						# Defining path to python interpreter</br>
						python_path = </br>
						# the directory to host coordinate information used to crop ROI </br>
						data_roi =</br>
						# dictionary directory </br>
						dict_dir= </code></p>
					</div>
						<p></p>
						<ul>
						<li><strong>Running Training</strong></li>
						</ul>
						<div class="ml1">
						<p><code>./run.sh 
						# options:</br>
						        --stage      -1  change the number to start from different training stages</br>
						        --nnet_stage -10 the number controls tdnn training stages including preprocessing and postprocessing</code></p>
						    </div>
						
						<ul>
						<li><strong>Other Tips</strong></li>
						</ul>
						<div class="ml1">
						<p>Here some naming rules for directories produced during the four TDNN models training processing</p>
						<table class='default'>
						<thead>
						<tr>
						<th>Models</th>
						<th>Data for Training</th>
						<th>Data for Alignment</th>
						<th>Model Directories</th>
						</tr>
						</thead>
						<tbody>
						<tr>
						<td>Chain-TDNN-A</td>
						<td>data/train_far_hires</td>
						<td>data/train_far</td>
						<td>exp/train_far</td>
						</tr>
						<tr>
						<td>Chain-TDNN-A*</td>
						<td>data/train_far_sp_hires</td>
						<td>data/train_far_sp</td>
						<td>exp/train_far_sp</td>
						</tr>
						<tr>
						<td>Chain-TDNN-AV</td>
						<td>data/train_far_hires_av</td>
						<td>data/train_far_av</td>
						<td>exp/train_far_av</td>
						</tr>
						<tr style="border-bottom: solid 1px;">
						<td>Chain-TDNN-AV*</td>
						<td>data/train_far_sp_hires_av</td>
						<td>data/train_far_sp_av</td>
						<td>exp/train_far_av_sp</td>
						</tr>
						</tbody>
						</table>
					</div>
					</div>
				</section> -->

				<section class="wrapper style1">
					<div class="container">
						<h1 style="font-size: 120%;">Requirments</h1>
						<!-- <ul>
						<li>
						<p><strong>Kaldi</strong></p>
						</li>
						<li>
						<p><strong>Python Packages:</strong></br>
						numpy</br>
						tqdm</br>
						<a href="https://github.com/fxsjy/jieba">jieba</a></p>
						</li>
						<li>
						<p><strong>Other Tools:</strong></br>
						<a href="https://github.com/fgnt/nara_wpe">nara_wpe</a></br>
						<a href="https://github.com/xanguera/BeamformIt">Beamformit</a></br>
						SRILM</br>
						<a href="https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks">Lipreading using Temporal Convolutional Networks</a></p>
						</li>
						</ul>
						<p></p> -->
					</div>
				</section>



<!-- 				<section class="wrapper style1">
					<div class="container">
						<h1 style="font-size: 150%;">Task2 Software</h1>
						<p></p>
					</div>
				</section>

				<section class="wrapper style2">
					<div class="container">
						<h1 style="font-size: 150%;">......</h1>
					</div>
				</section>

				<section class="wrapper style1">
					<div class="container">
						<h1 style="font-size: 150%;">......</h1>
					</div>
				</section> -->

			<!-- Footer -->
				<div id="footer">
					<!-- Copyright -->
						<div class="copyright">
							<ul class="menu">
								<li>&copy; All rights reserved</li><li>E-mail: mispchallenge@gmail.com</li>
							</ul>
						</div>

				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
